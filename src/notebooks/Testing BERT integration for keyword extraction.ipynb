{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing BERT integration for keyword extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacycake import BertKeyphraseExtraction as bake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_trf_bertbaseuncased_lg==2.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_trf_bertbaseuncased_lg-2.3.0/en_trf_bertbaseuncased_lg-2.3.0.tar.gz (405.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 405.8 MB 6.3 MB/s eta 0:00:01    |███▎                            | 41.1 MB 1.4 MB/s eta 0:04:15     |███▍                            | 43.3 MB 1.6 MB/s eta 0:03:49     |████▌                           | 57.4 MB 2.1 MB/s eta 0:02:43     |████████████▍                   | 156.8 MB 2.2 MB/s eta 0:01:54     |██████████████                  | 178.7 MB 2.1 MB/s eta 0:01:48     |████████████████▌               | 209.6 MB 1.6 MB/s eta 0:02:05eta 0:01:04     |█████████████████████▎          | 269.9 MB 2.2 MB/s eta 0:01:03     |███████████████████████████████▊| 401.7 MB 2.2 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from en_trf_bertbaseuncased_lg==2.3.0) (2.3.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (2.25.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (1.19.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (0.8.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (0.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (3.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (4.54.1)\n",
      "Requirement already satisfied: setuptools in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (51.0.0.post20201207)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (2.0.4)\n",
      "Requirement already satisfied: thinc==7.4.1 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (7.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (0.9.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (1.19.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (2.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (1.0.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (2.10)\n",
      "Collecting spacy-transformers<0.7.0,>=0.6.1\n",
      "  Downloading spacy_transformers-0.6.2-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy-transformers<0.7.0,>=0.6.1->en_trf_bertbaseuncased_lg==2.3.0) (1.6.0)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from en_trf_bertbaseuncased_lg==2.3.0) (2.3.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (1.0.5)\n",
      "Collecting ftfy<6.0.0,>=5.0.0\n",
      "  Downloading ftfy-5.8.tar.gz (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wcwidth in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from ftfy<6.0.0,>=5.0.0->spacy-transformers<0.7.0,>=0.6.1->en_trf_bertbaseuncased_lg==2.3.0) (0.2.5)\n",
      "Collecting pytokenizations>=0.2.0\n",
      "  Downloading pytokenizations-0.8.1-cp38-cp38-macosx_10_7_x86_64.whl (222 kB)\n",
      "\u001b[K     |████████████████████████████████| 222 kB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (0.9.6)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (1.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (0.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (4.54.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (0.8.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (1.19.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (2.0.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (3.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (1.0.5)\n",
      "Requirement already satisfied: future in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from torch>=1.0.0->spacy-transformers<0.7.0,>=0.6.1->en_trf_bertbaseuncased_lg==2.3.0) (0.18.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (1.19.2)\n",
      "Collecting torchcontrib<0.1.0,>=0.0.2\n",
      "  Downloading torchcontrib-0.0.2.tar.gz (11 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers<2.9.0,>=2.4.0\n",
      "  Downloading transformers-2.8.0-py3-none-any.whl (563 kB)\n",
      "\u001b[K     |████████████████████████████████| 563 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from transformers<2.9.0,>=2.4.0->spacy-transformers<0.7.0,>=0.6.1->en_trf_bertbaseuncased_lg==2.3.0) (3.0.12)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (2.25.0)\n",
      "Requirement already satisfied: sacremoses in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from transformers<2.9.0,>=2.4.0->spacy-transformers<0.7.0,>=0.6.1->en_trf_bertbaseuncased_lg==2.3.0) (0.0.43)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from transformers<2.9.0,>=2.4.0->spacy-transformers<0.7.0,>=0.6.1->en_trf_bertbaseuncased_lg==2.3.0) (2020.11.13)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (4.54.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (1.19.2)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.16.63-py2.py3-none-any.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.20.0,>=1.19.63\n",
      "  Downloading botocore-1.19.63-py2.py3-none-any.whl (7.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2 MB 580 kB/s eta 0:00:01     |█████████████████▋              | 4.0 MB 998 kB/s eta 0:00:04\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (1.25.11)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from botocore<1.20.0,>=1.19.63->boto3->transformers<2.9.0,>=2.4.0->spacy-transformers<0.7.0,>=0.6.1->en_trf_bertbaseuncased_lg==2.3.0) (2.8.1)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.63->boto3->transformers<2.9.0,>=2.4.0->spacy-transformers<0.7.0,>=0.6.1->en_trf_bertbaseuncased_lg==2.3.0) (1.15.0)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.4-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 843 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.63->boto3->transformers<2.9.0,>=2.4.0->spacy-transformers<0.7.0,>=0.6.1->en_trf_bertbaseuncased_lg==2.3.0) (1.15.0)\n",
      "Requirement already satisfied: click in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from sacremoses->transformers<2.9.0,>=2.4.0->spacy-transformers<0.7.0,>=0.6.1->en_trf_bertbaseuncased_lg==2.3.0) (7.1.2)\n",
      "Requirement already satisfied: joblib in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from sacremoses->transformers<2.9.0,>=2.4.0->spacy-transformers<0.7.0,>=0.6.1->en_trf_bertbaseuncased_lg==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from transformers<2.9.0,>=2.4.0->spacy-transformers<0.7.0,>=0.6.1->en_trf_bertbaseuncased_lg==2.3.0) (2020.11.13)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_trf_bertbaseuncased_lg==2.3.0) (4.54.1)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.95-cp38-cp38-macosx_10_6_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 686 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers==0.5.2\n",
      "  Downloading tokenizers-0.5.2-cp38-cp38-macosx_10_15_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 924 kB/s eta 0:00:01     |█████████▋                      | 337 kB 547 kB/s eta 0:00:02\n",
      "\u001b[?25hBuilding wheels for collected packages: en-trf-bertbaseuncased-lg, ftfy, torchcontrib\n",
      "  Building wheel for en-trf-bertbaseuncased-lg (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-trf-bertbaseuncased-lg: filename=en_trf_bertbaseuncased_lg-2.3.0-py3-none-any.whl size=405808566 sha256=dbe0a468eab012fa5680d4a678a44c4ca344383f09c9b2159c30ee7369b84463\n",
      "  Stored in directory: /private/var/folders/0p/m__g9kjx4r78_5877gj23wfw0000gn/T/pip-ephem-wheel-cache-bk567zbl/wheels/d5/7e/bb/f15c46ccd337a9e994c19ae0076ed59c445ef0dfcac3dd0083\n",
      "  Building wheel for ftfy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ftfy: filename=ftfy-5.8-py3-none-any.whl size=45613 sha256=f494dbc2842d56376dd7bc320115c6deb6afd5d2e280ba24aa5fcb662f83e83f\n",
      "  Stored in directory: /private/var/folders/0p/m__g9kjx4r78_5877gj23wfw0000gn/T/pip-ephem-wheel-cache-bk567zbl/wheels/3f/a5/65/684a672b6a26cb8ce3934d155c98d0e23b3dce3d2c0fadae19\n",
      "  Building wheel for torchcontrib (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torchcontrib: filename=torchcontrib-0.0.2-py3-none-any.whl size=7533 sha256=08fb3bd4348d4bf5b8ff1863c1f5679fad53140abc9b54945aca0f79c115cc24\n",
      "  Stored in directory: /private/var/folders/0p/m__g9kjx4r78_5877gj23wfw0000gn/T/pip-ephem-wheel-cache-bk567zbl/wheels/3b/75/17/b11b16ad90276ff6e4e03ec375d55291a186c7fe9dbf87fba3\n",
      "Successfully built en-trf-bertbaseuncased-lg ftfy torchcontrib\n",
      "Installing collected packages: jmespath, botocore, s3transfer, tokenizers, sentencepiece, boto3, transformers, torchcontrib, pytokenizations, ftfy, spacy-transformers, en-trf-bertbaseuncased-lg\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.9.4\n",
      "    Uninstalling tokenizers-0.9.4:\n",
      "      Successfully uninstalled tokenizers-0.9.4\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.2.2\n",
      "    Uninstalling transformers-4.2.2:\n",
      "      Successfully uninstalled transformers-4.2.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spacybert 1.0.1 requires transformers>=3.0.0, but you have transformers 2.8.0 which is incompatible.\u001b[0m\n",
      "Successfully installed boto3-1.16.63 botocore-1.19.63 en-trf-bertbaseuncased-lg-2.3.0 ftfy-5.8 jmespath-0.10.0 pytokenizations-0.8.1 s3transfer-0.3.4 sentencepiece-0.1.95 spacy-transformers-0.6.2 tokenizers-0.5.2 torchcontrib-0.0.2 transformers-2.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_trf_bertbaseuncased_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_trf_bertbaseuncased_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 140298491366416 acquired on /Users/joshuakraft/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /Users/joshuakraft/.cache/torch/transformers/tmpa9wrjcj1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6702f228d34ea8a7064e64b930cc12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /Users/joshuakraft/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO:transformers.file_utils:creating metadata file for /Users/joshuakraft/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO:filelock:Lock 140298491366416 released on /Users/joshuakraft/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/joshuakraft/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/joshuakraft/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:filelock:Lock 140296988971216 acquired on /Users/joshuakraft/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
      "INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /Users/joshuakraft/.cache/torch/transformers/tmp65wght7h\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d0034151354297a5c1adb75d619def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=440473133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin in cache at /Users/joshuakraft/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "INFO:transformers.file_utils:creating metadata file for /Users/joshuakraft/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "INFO:filelock:Lock 140296988971216 released on /Users/joshuakraft/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /Users/joshuakraft/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner', 'bert_keyphrase_extraction']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cake = bake(nlp, from_pretrained='bert-base-uncased', top_k=3)\n",
    "nlp.add_pipe(cake, last=True)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/joshuakraft/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages/en_core_web_lg/en_core_web_lg-2.3.1')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp._path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Deep learning (also known as deep structured learning) is part of a broader family \n",
    "        of machine learning methods based on artificial neural networks with representation \n",
    "        learning. Learning can be supervised, semi-supervised or unsupervised.[1][2][3]Deep-learning \n",
    "        architectures such as deep neural networks, deep belief networks, recurrent neural networks \n",
    "        and convolutional neural networks have been applied to fields including computer vision, \n",
    "        machine vision, speech recognition, natural language processing, audio recognition, social \n",
    "        network filtering, machine translation, bioinformatics, drug design, medical image analysis, \n",
    "        material inspection and board game programs, where they have produced results comparable \n",
    "        to and in some cases surpassing human expert performance.[4][5][6]Artificial neural networks \n",
    "        (ANNs) were inspired by information processing and distributed communication nodes in biological \n",
    "        systems. ANNs have various differences from biological brains. Specifically, neural networks tend \n",
    "        to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) \n",
    "        and analogue.[7][8][9]The adjective \"deep\" in deep learning refers to the use of multiple layers in \n",
    "        the network. Early work showed that a linear perceptron cannot be a universal classifier, and then \n",
    "        that a network with a nonpolynomial activation function with one hidden layer of unbounded width \n",
    "        can on the other hand so be. Deep learning is a modern variation which is concerned with an unbounded\n",
    "        number of layers of bounded size, which permits practical application and optimized implementation, \n",
    "        while retaining theoretical universality under mild conditions. In deep learning the layers are \n",
    "        also permitted to be heterogeneous and to deviate widely from biologically informed connectionist \n",
    "        models, for the sake of efficiency, trainability and understandability, whence the \"structured\" part.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cannot perform reduction function max on tensor with no elements because the operation does not have an identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0213d40c81e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__call__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages/spacycake/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mfirst_part\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphrases_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             second_part = torch.matmul(\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0mphrases_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 phrases_embeddings[S].transpose(0, 1)).max(dim=1).values\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot perform reduction function max on tensor with no elements because the operation does not have an identity"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc._.extracted_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeyBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_extractor = KeyBERT('distilbert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12fbdc25582d45f0ab1648691adf2d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Batches'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'DistilBertTokenizer' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6de464565853>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkeywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkw_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages/keybert/model.py\u001b[0m in \u001b[0;36mextract_keywords\u001b[0;34m(self, docs, keyphrase_ngram_range, stop_words, top_n, min_df, use_maxsum, use_mmr, diversity, nr_candidates, vectorizer)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             return self._extract_keywords_single_doc(docs,\n\u001b[0m\u001b[1;32m     88\u001b[0m                                                      \u001b[0mkeyphrase_ngram_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                                                      \u001b[0mstop_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages/keybert/model.py\u001b[0m in \u001b[0;36m_extract_keywords_single_doc\u001b[0;34m(self, doc, keyphrase_ngram_range, stop_words, top_n, use_maxsum, use_mmr, diversity, nr_candidates, vectorizer)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# Extract Embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mdoc_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0mword_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, is_pretokenized, device, num_workers)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstart_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0msentences_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mTokenizes\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \"\"\"\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_sentence_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TextAnalytics/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mto_tokenize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mto_tokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'longest_first'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DistilBertTokenizer' object is not callable"
     ]
    }
   ],
   "source": [
    "keywords = kw_extractor.extract_keywords(text, stop_words='english')\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TextAnalytics] *",
   "language": "python",
   "name": "conda-env-TextAnalytics-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
