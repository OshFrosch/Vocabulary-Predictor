{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit ('vocabulary-extraction': pipenv)",
   "metadata": {
    "interpreter": {
     "hash": "32307938ff14397177fbc24a45b6e68f6b7c4126eef78013cd3fb38c2647c0c8"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Exploring the arXMLiv dataset\n",
    "\n",
    "arXMLiv 08.2018 - An HTML5 dataset for arXiv.org Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_BASE_DIR = \"/Volumes/Backup/no_problem\""
   ]
  },
  {
   "source": [
    "Let's start by getting a overview over our dataset structure"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "directories = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(DATA_BASE_DIR):\n",
    "    for directory in d:\n",
    "        directories.append(os.path.join(r, directory))\n",
    "    for file in f:\n",
    "        if '.html' in file:\n",
    "            files.append(os.path.join(r, file))\n",
    "\n",
    "print(len(files))\n",
    "print(files[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(directories))\n",
    "print(directories[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " with open(files[0], \"rt\") as file:\n",
    "     print(file.read())"
   ]
  },
  {
   "source": [
    "We have 337 folders containing 150701 HTML5 documents taking up 60,25 gigabytes of storage. The only useful metadata associated with these files is their [arXiv-Identifier](https://arxiv.org/help/arxiv_identifier) which is used as the filename. \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Text extraction\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def read_file(file):\n",
    "    with open(file, \"rt\") as file:\n",
    "        return file.read()\n",
    "\n",
    "raw_file = read_file(files[0])\n",
    "soup = BeautifulSoup(raw_file, features=\"html.parser\")\n",
    "print(soup.get_text())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size reduction {:f}\".format(len(soup.get_text()) / len(raw_file)))"
   ]
  },
  {
   "source": [
    "We can extract the text relatively easily with BeautifulSoup. The text looks quite usable on the first glance and even this simple preprocessing dropped the size of the content down to 29% of the original size.\n",
    "But a closer look reveals artifacts like ```POSTSUBSCRIPT:start italic-nu POSTSUBSCRIPT:end OPEN:( italic-t CLOSE:)```. Additional postprocessing is needed.\n",
    "\n",
    "Let's have a look at the unusual html-tags in the file.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unusual_tags(soup):\n",
    "    # preload of some very common tags to reduce noise in the output\n",
    "    usual_tags = ['html', 'head', 'title', 'meta', 'body', 'div', 'article', 'p', 'section', 'span']\n",
    "    unusual_tag_types = []\n",
    "    unusual_content = []\n",
    "\n",
    "    for tag in soup.find_all():\n",
    "        if tag.name not in usual_tags:\n",
    "            if tag.name not in unusual_content:\n",
    "                unusual_tag_types.append(tag.name)\n",
    "                unusual_content.append(tag)\n",
    "            \n",
    "    print(unusual_tag_types)\n",
    "    print(unusual_content)\n",
    "\n",
    "unusual_tags(soup)\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "Skimming the list of ununusal tags shows the repeated occurence of ```MathMl```-tags. Dropping these should further clean up the text."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(read_file(files[0]), features=\"html.parser\")\n",
    "for script in soup([\"math\"]):\n",
    "    script.extract()\n",
    "\n",
    "unusual_tags(soup)\n",
    "\n",
    "reduced_text = soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size reduction {:f}\".format(len(soup.get_text()) / len(raw_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}