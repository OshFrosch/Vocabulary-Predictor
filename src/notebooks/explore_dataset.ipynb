{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit ('vocabulary-extraction': pipenv)",
   "metadata": {
    "interpreter": {
     "hash": "32307938ff14397177fbc24a45b6e68f6b7c4126eef78013cd3fb38c2647c0c8"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Exploring the arXMLiv dataset\n",
    "\n",
    "arXMLiv 08.2018 - An HTML5 dataset for arXiv.org Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_BASE_DIR = \"/Volumes/Backup/no_problem\""
   ]
  },
  {
   "source": [
    "Let's start by getting a overview over our dataset structure"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "directories = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(DATA_BASE_DIR):\n",
    "    for directory in d:\n",
    "        directories.append(os.path.join(r, directory))\n",
    "    for file in f:\n",
    "        if '.html' in file:\n",
    "            files.append(os.path.join(r, file))\n",
    "\n",
    "number_of_files = len(files)\n",
    "\n",
    "\n",
    "print(number_of_files)\n",
    "print(files[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(directories))\n",
    "print(directories[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " with open(files[0], \"rt\") as file:\n",
    "     print(file.read())"
   ]
  },
  {
   "source": [
    "We have 337 folders containing 150701 HTML5 documents taking up 60,25 gigabytes of storage. The only useful metadata associated with these files is their [arXiv-Identifier](https://arxiv.org/help/arxiv_identifier) which is used as the filename. \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Text extraction\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def read_file(file):\n",
    "    with open(file, \"rt\") as file:\n",
    "        return file.read()\n",
    "\n",
    "raw_file = read_file(files[0])\n",
    "soup = BeautifulSoup(raw_file, features=\"html.parser\")\n",
    "print(soup.get_text())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size reduction {:f}\".format(len(soup.get_text()) / len(raw_file)))"
   ]
  },
  {
   "source": [
    "We can extract the text relatively easily with BeautifulSoup. The text looks quite usable on the first glance and even this simple preprocessing dropped the size of the content down to 29% of the original size.\n",
    "But a closer look reveals artifacts like ```POSTSUBSCRIPT:start italic-nu POSTSUBSCRIPT:end OPEN:( italic-t CLOSE:)```. Additional postprocessing is needed.\n",
    "\n",
    "Let's have a look at the unusual html-tags in the file.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unusual_tags(soup):\n",
    "    # preload of some very common tags to reduce noise in the output\n",
    "    usual_tags = ['html', 'head', 'title', 'meta', 'body', 'div', 'article', 'p', 'section', 'span']\n",
    "    unusual_tag_types = []\n",
    "    unusual_content = []\n",
    "\n",
    "    for tag in soup.find_all():\n",
    "        if tag.name not in usual_tags:\n",
    "            if tag.name not in unusual_content:\n",
    "                unusual_tag_types.append(tag.name)\n",
    "                unusual_content.append(tag)\n",
    "            \n",
    "    print(unusual_tag_types)\n",
    "    print(unusual_content)\n",
    "\n",
    "unusual_tags(soup)\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "Skimming the list of ununusal tags shows the repeated occurence of ```MathMl```-tags and the footer only containing the note that LateXml was used for the conversion. Dropping these should further clean up the text."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(read_file(files[0]), features=\"html.parser\")\n",
    "for script in soup([\"math\", \"footer\"]):\n",
    "    script.extract()\n",
    "\n",
    "unusual_tags(soup)\n",
    "\n",
    "reduced_text = soup.get_text()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size reduction {:f}\".format(len(soup.get_text()) / len(raw_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.get_text())"
   ]
  },
  {
   "source": [
    "## Combining the techniques \n",
    "\n",
    "Let's use these techniques and combine them with some normal string cleaning to create a cleanup function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_text(file):\n",
    "    soup = BeautifulSoup(read_file(file), features=\"html.parser\")\n",
    "    for script in soup([\"math\", \"footer\"]):\n",
    "        script.extract()\n",
    "    text = soup.get_text()\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(r\"n't|'[A-Za-z]+\", '', text) # drop the all contrations since they are all stopwords I've -> I\n",
    "    text = re.sub(r'[^a-zA-Z\\s:]', '', text) # drop non alphabetic characters\n",
    "    text = re.sub(\" +\", \" \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "#print(get_text(files[0]))\n",
    "\n",
    "\n",
    "for file in files[0:10]:\n",
    "    get_text(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import re\n",
    "import sys\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable = ['ner', 'tagger', 'parser']) # and disable the tagger, parser and ner.\n",
    "stopwords = nlp.Defaults.stop_words # load the list of stopwords from spacy for the English language\n",
    "stemmer = PorterStemmer() ### initialize the stemmer from NLTK\n",
    "\n",
    "def remove_stopwords_and_stem(text):\n",
    "    tokens_without_stopwords = [token for token in nlp(text) if not token.text in stopwords]\n",
    "    stemmed_tokens = [stemmer.stem(token.text) for token in tokens_without_stopwords if len(token.text) > 1]\n",
    "    text = \" \".join(stemmed_tokens)\n",
    "    return(re.sub(\"\\s{2,}\", \" \", text))\n",
    "\n",
    "print(remove_stopwords_and_stem(get_text(files[0])))\n",
    "        "
   ]
  },
  {
   "source": [
    "## Gather information about the dataset\n",
    "\n",
    "Let's apply the created cleanup functions onto a subset of the documents and get a feeling for the dataset by running some analysis and trying to model the properties of the complete dataset.\n",
    "We use the random.sample()-function with a fixed seed to get repeatable results.\n",
    "\n",
    "A pickeld result of analysing 5000 documents is stored alongside this notbook to speed up execution."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "import random \n",
    "import pandas as pd\n",
    "\n",
    "AMOUNT_OF_DOCUMENTS = 10\n",
    "SEED=42\n",
    "random.seed(SEED)\n",
    "chosen_files = random.sample(files,AMOUNT_OF_DOCUMENTS)\n",
    "nlp.max_length =2000000 \n",
    "\n",
    "rows = []\n",
    "\n",
    "complete_counter = Counter()\n",
    "\n",
    "for index, file in enumerate(chosen_files):\n",
    "    raw_file = read_file(file)\n",
    "    raw_length = len(raw_file)\n",
    "    \n",
    "\n",
    "    text = get_text(file)\n",
    "    text_length = len(text)\n",
    "    \n",
    "    cleaned_text = remove_stopwords_and_stem(text)\n",
    "    cleaned_length = len(cleaned_text)\n",
    "\n",
    "    tokens = cleaned_text.split(\" \")\n",
    "\n",
    "    amount_of_words = len(tokens)\n",
    "\n",
    "    counter = Counter()\n",
    "    counter.update(tokens)\n",
    "    unique_words = len(counter.keys())\n",
    "\n",
    "    complete_counter.update(counter)\n",
    "    total_words=len(complete_counter.keys())\n",
    "\n",
    "    row = [raw_length, text_length, cleaned_length, amount_of_words, unique_words, total_words ]\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"raw_length\", \"text_length\", \"cleaned_length\", \"amount_of_words\", \"unique_words\", \"total_words\"])\n",
    "df\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_pickle(\"results_5000_documents_seed_42\")\n",
    "\n",
    "def compression_factor(cleaned, raw):\n",
    "    return (cleaned/raw)\n",
    "\n",
    "df['compression_factor'] = compression_factor(df['text_length'], df['raw_length'])\n",
    "df"
   ]
  },
  {
   "source": [
    "We are meassuring different properties of the dataset.\n",
    "The ```_length``-fields store the string length in characters after reading the file, cleaning it and removing the stopwords and tokenizing it.\n",
    "\n",
    "```amount_of_words```, ```unique_words``` and ```total_words``` are using the stemmed strings. ```total_words``` is more than a sum of ```unique_words```. It is the amount of keys stored in a ```collections.Counter```-instance that is used for all files.\n",
    "\n",
    "Looking at the distributions shows nothing unexpected. Every attribute is normaly distributed with a long tail to the right.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax = df.loc[:, df.columns != 'total_words'].hist(layout=(6,1),bins=100, figsize=(8,15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "word_length = [len(x) for x in complete_counter.keys()]\n",
    "word_length_df = pd.DataFrame(word_length)\n",
    "\n",
    "word_length_df.hist(bins=max(word_length)-min(word_length))\n",
    "plt.title(\"token length distribution\")\n"
   ]
  },
  {
   "source": [
    "The amount of unique tokens seems to resemble a logarithmic growth curve. This matches the intuition that each additional document can only add so many new token to the set of already seen tokens.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "df.loc[:, df.columns == 'total_words'].plot.line(layout=(1,1), figsize=(10,6))\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "yn=df.loc[:, df.columns == 'total_words'].values.flatten()\n",
    "\n",
    "x=range(5000)\n",
    "\n",
    "def logFunc( x, a, b, c, d ):\n",
    "    return a*np.log( b*x + c ) + d\n",
    "\n",
    "popt, pcov = curve_fit(logFunc, x, yn)\n",
    "\n",
    "print(popt)\n",
    "\n",
    "plt.plot(x,yn, label=\"Original data\")\n",
    "plt.plot(x, logFunc(x,popt[0],popt[1],popt[2],popt[3]), label=\"Fitted curve\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Unique tokens prediction')\n",
    "plt.xlabel('Number of documents')\n",
    "plt.ylabel('Number of unique tokens')\n",
    "plt.show()\n",
    "\n",
    "def predict_unique_token_count(x):\n",
    "    return logFunc(x, popt[0],popt[1],popt[2],popt[3])\n",
    "\n"
   ]
  },
  {
   "source": [
    "We can fit a function and try to estimate the total amount of unique tokens in the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = predict_unique_token_count(number_of_files)\n",
    "print(\"The number of unique tokens for the complete dataset is modeled to be around {}\".format(unique_tokens))"
   ]
  },
  {
   "source": [
    "We can repeat this approach to get an estimation of the total amount of tokens contained in the dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, df.columns == 'amount_of_words'].cumsum().plot.line(layout=(1,1), figsize=(10,6))\n",
    "yn=df.loc[:, df.columns == 'amount_of_words'].cumsum().values.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.polynomial import Polynomial\n",
    "\n",
    "cmin, cmax = min(x), max(x)\n",
    "pfit, stats = Polynomial.fit(x, yn, 1, full=True, window=(cmin, cmax),\n",
    "                                                    domain=(cmin, cmax))\n",
    "\n",
    "plt.plot(x,yn, label=\"Original data\")\n",
    "plt.plot(x, pfit(x), label=\"Fitted curve\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Amount of tokens')\n",
    "plt.xlabel('Number of documents')\n",
    "plt.ylabel('Number of tokens')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_of_tokens = pfit(unique_tokens)\n",
    "print(\"The number of unique tokens for the complete dataset is modeled to be around {}\".format(unique_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}