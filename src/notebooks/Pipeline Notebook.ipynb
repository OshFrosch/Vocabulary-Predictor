{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Project Pipline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "from spacy.tokens import Doc\n",
    "from spacy.language import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordfreq import zipf_frequency\n",
    "from wordfreq import word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('cmudict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import cmudict\n",
    "\n",
    "phoneme_dict = dict(cmudict.entries())\n",
    "\n",
    "def syllable_counter(word):\n",
    "    '''function that counts a syllable in a word'''\n",
    "    if word not in phoneme_dict:\n",
    "        return 0\n",
    "    syllables = phoneme_dict[word]\n",
    "    count = len([syllable for syllable in syllables if syllable[-1].isdigit()])\n",
    "    return round(count, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Minimum():\n",
    "    def __init__(self, initial_value):\n",
    "        self.value = initial_value\n",
    "    \n",
    "    def update_minimum(self, potential_min):\n",
    "        if potential_min < self.value:\n",
    "            self.value = potential_min\n",
    "            \n",
    "class Maximum():\n",
    "    def __init__(self, initial_value):\n",
    "        self.value = initial_value\n",
    "    \n",
    "    def update_maximum(self, potential_min):\n",
    "        if potential_min > self.value:\n",
    "            self.value = potential_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.0\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = '../data/kafka.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(open(text_file, \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcount(doc):\n",
    "    '''gives an overall word count'''\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.is_alpha:\n",
    "            doc._.wordcount += 1\n",
    "    \n",
    "    print(f'{doc._.wordcount} words in document')\n",
    "         \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tokens(doc):\n",
    "    '''filters all tokens'''           \n",
    "    \n",
    "    \n",
    "    for token in doc:\n",
    "        # filter stopwords\n",
    "        if not token.is_alpha or token.is_stop:\n",
    "            token._.is_excluded = True\n",
    "            \n",
    "        # filter part-of-speech\n",
    "        elif token.pos_ not in ['NOUN', 'VERB', 'ADJ']:\n",
    "            token._.is_excluded = True\n",
    "            \n",
    "        # filter entities\n",
    "        elif token.ent_type != 0:\n",
    "                token._.is_excluded = True\n",
    "                \n",
    "        # count included words   \n",
    "        else:\n",
    "            doc._.included_wordcount += 1\n",
    "\n",
    "    print(f'{doc._.included_wordcount} words in vocabulary')\n",
    "         \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminating Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elim_dup(doc):\n",
    "    '''eliminates all duplicates and counts the appearance of the included words'''\n",
    "    already_appeared = {}\n",
    "\n",
    "    for token in doc:\n",
    "        if not token._.is_excluded:\n",
    "            if token.lemma_ in already_appeared.keys():\n",
    "                already_appeared[token.lemma_]._.appearance += 1\n",
    "                token._.is_excluded = True\n",
    "                doc._.included_wordcount -= 1\n",
    "            else:\n",
    "                token._.appearance = 1\n",
    "                already_appeared[token.lemma_] = token\n",
    "    \n",
    "    print(f'{doc._.included_wordcount} words in vocabulary without duplicates')\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Difficulty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rating Difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syl_weight(n):\n",
    "    w = 0\n",
    "    for i in range(n):\n",
    "        w += 0.5**(i+1)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_difficulty(doc):\n",
    "   \n",
    "    for token in doc:\n",
    "        if not token._.is_excluded:\n",
    "            lemma = token.lemma_\n",
    "            difficulty = 8 - zipf_frequency(lemma, 'en') # score of 0-8\n",
    "            difficulty += syl_weight(syllable_counter(lemma)) # now score of 0-9\n",
    "            token._.difficulty = round(difficulty/9, 3) #normalised to 0-1\n",
    "            \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Freqency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_relativ_freq(doc):\n",
    "    '''calculating the relativ frequency of a included word'''\n",
    "    \n",
    "    calculate_last = []\n",
    "    min_freq = Minimum(1)\n",
    "    max_score = Maximum(0)\n",
    "    \n",
    "    def calc_rel_freq(word_freq, token):\n",
    "        return round(((token._.appearance/doc._.wordcount) **2) / word_freq, 3)\n",
    "    \n",
    "    for token in doc:\n",
    "        if not token._.is_excluded:\n",
    "            overall_word_freq = word_frequency(token.lemma_, 'en')\n",
    "            \n",
    "            if overall_word_freq == 0:\n",
    "                calculate_last.append(token)\n",
    "            else:\n",
    "                min_freq.update_minimum(overall_word_freq)\n",
    "                token._.relativ_freq = calc_rel_freq(overall_word_freq, token)\n",
    "                max_score.update_maximum(token._.relativ_freq)\n",
    "    \n",
    "    for token in calculate_last:\n",
    "        token._.relativ_freq = calc_rel_freq(min_freq.value, token)\n",
    "        max_score.update_maximum(token._.relativ_freq)\n",
    "        \n",
    "    for token in doc:\n",
    "        if not token._.is_excluded:\n",
    "            token._.relativ_freq /= max_score.value\n",
    "    \n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yake import KeywordExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_keyphrases(doc):\n",
    "    \n",
    "    kw_extractor = KeywordExtractor(lan=\"en\", n=1, top=100)\n",
    "    kw = dict(kw_extractor.extract_keywords(doc.text))\n",
    "    \n",
    "    keywords_token = []\n",
    "    already_in_list = []\n",
    "    for token in doc:\n",
    "        if not token._.is_excluded:\n",
    "            if token.lemma_ in kw.keys():\n",
    "                token._.keyword_score = 1 - kw[token.lemma_]\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"         \\n        if token.text in kw:\\n            token._.is_keyword = True\\n            if token.text not in already_in_list:\\n                keywords_token.append(token)\\n                already_in_list.append(token.text)\\n    doc._.keywords = keywords_token\\n    \\n    print(f'{len(keywords_token)} words are keywords')\\n    \\n    for token in doc:\\n        for kw in keywords_token:\\n            kw_score = token.similarity(kw)\\n            if kw_score > token._.keyword_score:\\n                token._.keyword_score =  kw_score\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''         \n",
    "        if token.text in kw:\n",
    "            token._.is_keyword = True\n",
    "            if token.text not in already_in_list:\n",
    "                keywords_token.append(token)\n",
    "                already_in_list.append(token.text)\n",
    "    doc._.keywords = keywords_token\n",
    "    \n",
    "    print(f'{len(keywords_token)} words are keywords')\n",
    "    \n",
    "    for token in doc:\n",
    "        for kw in keywords_token:\n",
    "            kw_score = token.similarity(kw)\n",
    "            if kw_score > token._.keyword_score:\n",
    "                token._.keyword_score =  kw_score'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline():\n",
    "    nlp = spacy.load('en_core_web_lg', disable = ['parser'])\n",
    "    \n",
    "    ## Preprocessing\n",
    "    # wordcount\n",
    "    Doc.set_extension('wordcount', default=0, force=True)\n",
    "    nlp.add_pipe(wordcount)\n",
    "    \n",
    "    # filter tokens\n",
    "    Doc.set_extension('included_wordcount', default=0, force=True)\n",
    "    Token.set_extension('is_excluded', default=False, force=True)\n",
    "    nlp.add_pipe(filter_tokens)\n",
    "    \n",
    "    # eliminate dublicates\n",
    "    Token.set_extension('appearance', default=np.nan, force=True)\n",
    "    nlp.add_pipe(elim_dup)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Word difficulty\n",
    "    # difficulty\n",
    "    Token.set_extension('difficulty', default=0, force=True)\n",
    "    nlp.add_pipe(get_difficulty)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Word Relevance\n",
    "    # relative frequency\n",
    "    Token.set_extension('relativ_freq', default=np.nan, force=True)\n",
    "    nlp.add_pipe(calculate_relativ_freq)\n",
    "    \n",
    "    # keywordscore\n",
    "    Doc.set_extension('keywords', default=[], force=True)\n",
    "    Token.set_extension('is_keyword', default=False, force=True)\n",
    "    Token.set_extension('keyword_score', default=0, force=True)\n",
    "    nlp.add_pipe(check_keyphrases)\n",
    "    \n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger',\n",
       " 'ner',\n",
       " 'wordcount',\n",
       " 'filter_tokens',\n",
       " 'elim_dup',\n",
       " 'get_difficulty',\n",
       " 'calculate_relativ_freq',\n",
       " 'check_keyphrases']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = create_pipeline()\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_doc(doc):\n",
    "    data = []\n",
    "    for token in doc:\n",
    "        if not token._.is_excluded:\n",
    "            data.append((token, token.lemma_, token._.appearance, token._.difficulty, token._.relativ_freq, token._.keyword_score))\n",
    "    df = pd.DataFrame(data, columns=['token', 'lemma', 'appearance', 'difficulty', 'relative freqency', 'keyword score'])\n",
    "    \n",
    "    df[['difficulty_rank', 'keyword_rank']] = df[['difficulty', 'keyword score']].rank(ascending=False)\n",
    "    \n",
    "    df['overall_ranking'] = 3 * df['difficulty'] + df['relative freqency'] + df['keyword score']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_outputsize(df):\n",
    "    \n",
    "    def user_knows_vocab(s):\n",
    "        response = input(f'Can you translate this word:   {s}   [y/n]  ')\n",
    "        \n",
    "        while response not in ['y','n']:\n",
    "            response = input('Unexpected input. Try again.\\n' +\n",
    "                             f'Can you translate this word:   {s}   [y/n]  ')\n",
    "        return response == 'y'\n",
    "    \n",
    "    \n",
    "    n = len(df)\n",
    "    dict_word_known = {}\n",
    "    result = False\n",
    "    \n",
    "    i = 1\n",
    "    loc = 0\n",
    "    offset = 0\n",
    "    last = False\n",
    "    while loc <= n:\n",
    "        \n",
    "        print(f'Word position {loc}:')\n",
    "        if loc in dict_word_known.keys():\n",
    "            result = dict_word_known[loc]\n",
    "        else:\n",
    "            result = user_knows_vocab(df.iloc[loc]['lemma'])\n",
    "            dict_word_known[loc] = result\n",
    "        \n",
    "        if result:\n",
    "            if last:\n",
    "                break\n",
    "            last = True\n",
    "            offset = last_loc\n",
    "            i = 0\n",
    "        else:\n",
    "            last = False\n",
    "            \n",
    "        i += 1 \n",
    "        last_loc = loc\n",
    "        loc = (2**i) - 1 + offset\n",
    "            \n",
    "    \n",
    "    return df[:last_loc-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word position 0:\n",
      "Can you translate this word:   deep   [y/n]  n\n",
      "Word position 3:\n",
      "Can you translate this word:   structured   [y/n]  n\n",
      "Word position 7:\n",
      "Can you translate this word:   method   [y/n]  n\n",
      "Word position 15:\n",
      "Can you translate this word:   learn   [y/n]  y\n",
      "Word position 8:\n",
      "Can you translate this word:   base   [y/n]  n\n",
      "Word position 10:\n",
      "Can you translate this word:   neural   [y/n]  n\n",
      "Word position 14:\n",
      "Can you translate this word:   supervised   [y/n]  n\n",
      "Word position 22:\n",
      "Can you translate this word:   include   [y/n]  y\n",
      "Word position 15:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>appearance</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>relative freqency</th>\n",
       "      <th>keyword score</th>\n",
       "      <th>difficulty_rank</th>\n",
       "      <th>keyword_rank</th>\n",
       "      <th>overall_ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Deep</td>\n",
       "      <td>deep</td>\n",
       "      <td>8</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.015001</td>\n",
       "      <td>0.955717</td>\n",
       "      <td>91.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.140719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>learning</td>\n",
       "      <td>learning</td>\n",
       "      <td>7</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.012882</td>\n",
       "      <td>0.944046</td>\n",
       "      <td>75.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.225928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>known</td>\n",
       "      <td>know</td>\n",
       "      <td>1</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>110.0</td>\n",
       "      <td>87.5</td>\n",
       "      <td>0.798018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>structured</td>\n",
       "      <td>structured</td>\n",
       "      <td>2</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.012652</td>\n",
       "      <td>0.736233</td>\n",
       "      <td>25.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.377885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>broader</td>\n",
       "      <td>broad</td>\n",
       "      <td>1</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>72.0</td>\n",
       "      <td>87.5</td>\n",
       "      <td>1.299575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>family</td>\n",
       "      <td>family</td>\n",
       "      <td>1</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.790779</td>\n",
       "      <td>103.5</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1.822830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>machine</td>\n",
       "      <td>machine</td>\n",
       "      <td>3</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.002595</td>\n",
       "      <td>0.902369</td>\n",
       "      <td>73.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.188964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>methods</td>\n",
       "      <td>method</td>\n",
       "      <td>1</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>71.0</td>\n",
       "      <td>87.5</td>\n",
       "      <td>1.308339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>based</td>\n",
       "      <td>base</td>\n",
       "      <td>1</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93.0</td>\n",
       "      <td>87.5</td>\n",
       "      <td>1.158213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>artificial</td>\n",
       "      <td>artificial</td>\n",
       "      <td>1</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.001449</td>\n",
       "      <td>0.912344</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.491793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>neural</td>\n",
       "      <td>neural</td>\n",
       "      <td>6</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.197697</td>\n",
       "      <td>0.960700</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.868397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>networks</td>\n",
       "      <td>network</td>\n",
       "      <td>10</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.020818</td>\n",
       "      <td>0.873967</td>\n",
       "      <td>79.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.130785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>representation</td>\n",
       "      <td>representation</td>\n",
       "      <td>1</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.790779</td>\n",
       "      <td>37.5</td>\n",
       "      <td>10.5</td>\n",
       "      <td>2.315691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>supervised</td>\n",
       "      <td>supervise</td>\n",
       "      <td>1</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.009786</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.5</td>\n",
       "      <td>87.5</td>\n",
       "      <td>1.845786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>supervised</td>\n",
       "      <td>supervised</td>\n",
       "      <td>1</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.005241</td>\n",
       "      <td>0.564295</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>2.315536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>learning</td>\n",
       "      <td>learn</td>\n",
       "      <td>2</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>97.0</td>\n",
       "      <td>87.5</td>\n",
       "      <td>1.113632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>architectures</td>\n",
       "      <td>architecture</td>\n",
       "      <td>1</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.0</td>\n",
       "      <td>87.5</td>\n",
       "      <td>1.506871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>belief</td>\n",
       "      <td>belief</td>\n",
       "      <td>1</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.521135</td>\n",
       "      <td>54.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.937859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>recurrent</td>\n",
       "      <td>recurrent</td>\n",
       "      <td>1</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.009786</td>\n",
       "      <td>0.564295</td>\n",
       "      <td>14.5</td>\n",
       "      <td>20.5</td>\n",
       "      <td>2.410081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>convolutional</td>\n",
       "      <td>convolutional</td>\n",
       "      <td>1</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.489316</td>\n",
       "      <td>0.521135</td>\n",
       "      <td>7.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.119451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>applied</td>\n",
       "      <td>apply</td>\n",
       "      <td>1</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.0</td>\n",
       "      <td>87.5</td>\n",
       "      <td>1.329398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             token           lemma  appearance  difficulty  relative freqency  \\\n",
       "0             Deep            deep           8       0.390           0.015001   \n",
       "1         learning        learning           7       0.423           0.012882   \n",
       "2            known            know           1       0.266           0.000018   \n",
       "3       structured      structured           2       0.543           0.012652   \n",
       "4          broader           broad           1       0.433           0.000575   \n",
       "5           family          family           1       0.344           0.000051   \n",
       "6          machine         machine           3       0.428           0.002595   \n",
       "7          methods          method           1       0.436           0.000339   \n",
       "8            based            base           1       0.386           0.000213   \n",
       "9       artificial      artificial           1       0.526           0.001449   \n",
       "10          neural          neural           6       0.570           0.197697   \n",
       "11        networks         network          10       0.412           0.020818   \n",
       "12  representation  representation           1       0.508           0.000912   \n",
       "13      supervised       supervise           1       0.612           0.009786   \n",
       "14      supervised      supervised           1       0.582           0.005241   \n",
       "15        learning           learn           2       0.371           0.000632   \n",
       "16   architectures    architecture           1       0.502           0.000871   \n",
       "17          belief          belief           1       0.472           0.000724   \n",
       "18       recurrent       recurrent           1       0.612           0.009786   \n",
       "19   convolutional   convolutional           1       0.703           0.489316   \n",
       "20         applied           apply           1       0.443           0.000398   \n",
       "\n",
       "    keyword score  difficulty_rank  keyword_rank  overall_ranking  \n",
       "0        0.955717             91.5           2.0         2.140719  \n",
       "1        0.944046             75.0           3.0         2.225928  \n",
       "2        0.000000            110.0          87.5         0.798018  \n",
       "3        0.736233             25.0          13.0         2.377885  \n",
       "4        0.000000             72.0          87.5         1.299575  \n",
       "5        0.790779            103.5          10.5         1.822830  \n",
       "6        0.902369             73.0           5.0         2.188964  \n",
       "7        0.000000             71.0          87.5         1.308339  \n",
       "8        0.000000             93.0          87.5         1.158213  \n",
       "9        0.912344             32.0           4.0         2.491793  \n",
       "10       0.960700             21.0           1.0         2.868397  \n",
       "11       0.873967             79.0           6.0         2.130785  \n",
       "12       0.790779             37.5          10.5         2.315691  \n",
       "13       0.000000             14.5          87.5         1.845786  \n",
       "14       0.564295             18.0          20.5         2.315536  \n",
       "15       0.000000             97.0          87.5         1.113632  \n",
       "16       0.000000             41.0          87.5         1.506871  \n",
       "17       0.521135             54.0          34.0         1.937859  \n",
       "18       0.564295             14.5          20.5         2.410081  \n",
       "19       0.521135              7.0          34.0         3.119451  \n",
       "20       0.000000             64.0          87.5         1.329398  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_outputsize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test scentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 words in document\n",
      "9 words in vocabulary\n",
      "6 words in vocabulary without duplicates\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Hi I'm a little little boy. Get me a piece of cake or I'll killed your mother. I am the mother of your mother\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_from_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['overall_ranking'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>appearance</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>relative freqency</th>\n",
       "      <th>keyword score</th>\n",
       "      <th>difficulty_rank</th>\n",
       "      <th>keyword_rank</th>\n",
       "      <th>overall_ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [token, lemma, appearance, difficulty, relative freqency, keyword score, difficulty_rank, keyword_rank, overall_ranking]\n",
       "Index: []"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25062 words in document\n",
      "7812 words in vocabulary\n",
      "1873 words in vocabulary without duplicates\n"
     ]
    }
   ],
   "source": [
    "with open(text_file, \"r\") as f:\n",
    "    doc = nlp(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_from_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['overall_ranking'], ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word position 0:\n",
      "Can you translate this word:   charwoman   [y/n]  n\n",
      "Word position 3:\n",
      "Can you translate this word:   swinge   [y/n]  n\n",
      "Word position 7:\n",
      "Can you translate this word:   ebook   [y/n]  y\n",
      "Word position 4:\n",
      "Can you translate this word:   teaboy   [y/n]  y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>appearance</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>relative freqency</th>\n",
       "      <th>keyword score</th>\n",
       "      <th>difficulty_rank</th>\n",
       "      <th>keyword_rank</th>\n",
       "      <th>overall_ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>charwoman</td>\n",
       "      <td>charwoman</td>\n",
       "      <td>8</td>\n",
       "      <td>0.807</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>964.0</td>\n",
       "      <td>3.421000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>fretsaw</td>\n",
       "      <td>fretsaw</td>\n",
       "      <td>1</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.053115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>964.0</td>\n",
       "      <td>2.720115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>soughing</td>\n",
       "      <td>soughing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.053115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>964.0</td>\n",
       "      <td>2.720115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>swinging</td>\n",
       "      <td>swinge</td>\n",
       "      <td>1</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.053115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>964.0</td>\n",
       "      <td>2.720115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1358</th>\n",
       "      <td>teaboy</td>\n",
       "      <td>teaboy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.053115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>964.0</td>\n",
       "      <td>2.720115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>startlement</td>\n",
       "      <td>startlement</td>\n",
       "      <td>1</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.053115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>964.0</td>\n",
       "      <td>2.720115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            token        lemma  appearance  difficulty  relative freqency  \\\n",
       "1335    charwoman    charwoman           8       0.807           1.000000   \n",
       "497       fretsaw      fretsaw           1       0.889           0.053115   \n",
       "655      soughing     soughing           1       0.889           0.053115   \n",
       "396      swinging       swinge           1       0.889           0.053115   \n",
       "1358       teaboy       teaboy           1       0.889           0.053115   \n",
       "1166  startlement  startlement           1       0.889           0.053115   \n",
       "\n",
       "      keyword score  difficulty_rank  keyword_rank  overall_ranking  \n",
       "1335            0.0              7.0         964.0         3.421000  \n",
       "497             0.0              3.0         964.0         2.720115  \n",
       "655             0.0              3.0         964.0         2.720115  \n",
       "396             0.0              3.0         964.0         2.720115  \n",
       "1358            0.0              3.0         964.0         2.720115  \n",
       "1166            0.0              3.0         964.0         2.720115  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_outputsize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordclusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 words in document\n",
      "32 words in vocabulary\n",
      "22 words in vocabulary without duplicates\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Let's take a ride in my new car. I will drive you home in my car. I love cars, which drive very fast and loud. Trucks and vans are the best. You can also cook in the back of my car. Food, meals and drinks are very tasty. My mother is all in to cooking good meals, foods and healthy dishes. This keeps me healthy while. I love the pasta and pies she bakes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_from_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>appearance</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>relative freqency</th>\n",
       "      <th>keyword score</th>\n",
       "      <th>word vector</th>\n",
       "      <th>difficulty_rank</th>\n",
       "      <th>keyword_rank</th>\n",
       "      <th>overall_ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tasty</td>\n",
       "      <td>tasty</td>\n",
       "      <td>1</td>\n",
       "      <td>0.551</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.362640</td>\n",
       "      <td>[-0.5076, -0.32474, 0.44828, 0.18926, 0.017181...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.015640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pasta</td>\n",
       "      <td>pasta</td>\n",
       "      <td>1</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.852215</td>\n",
       "      <td>0.259213</td>\n",
       "      <td>[-0.47621, -0.12565, 0.57201, -0.41622, 0.0878...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.740427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bakes</td>\n",
       "      <td>bake</td>\n",
       "      <td>1</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.912728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[-0.027228, 0.17126, 0.30655, -0.4484, -0.0939...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>2.469728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>healthy</td>\n",
       "      <td>healthy</td>\n",
       "      <td>2</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.409298</td>\n",
       "      <td>0.656656</td>\n",
       "      <td>[-0.40781, 0.53036, -0.25297, 0.19766, -0.1103...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.388954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>car</td>\n",
       "      <td>car</td>\n",
       "      <td>4</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.375376</td>\n",
       "      <td>0.898871</td>\n",
       "      <td>[0.20987, 0.46481, -0.24238, -0.065751, 0.6085...</td>\n",
       "      <td>16.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.300247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cook</td>\n",
       "      <td>cook</td>\n",
       "      <td>2</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.577998</td>\n",
       "      <td>0.314925</td>\n",
       "      <td>[-0.34052, -0.24223, 0.5442, -0.069727, 0.0716...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.182923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>meals</td>\n",
       "      <td>meal</td>\n",
       "      <td>2</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.836605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[-0.32314, -0.24613, 0.41368, -0.28846, -0.097...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>2.180605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ride</td>\n",
       "      <td>ride</td>\n",
       "      <td>1</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.097780</td>\n",
       "      <td>0.764928</td>\n",
       "      <td>[0.39289, -0.088632, -0.30353, -0.47467, 0.313...</td>\n",
       "      <td>11.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.095708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>drive</td>\n",
       "      <td>drive</td>\n",
       "      <td>2</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.200662</td>\n",
       "      <td>0.737823</td>\n",
       "      <td>[0.57215, 0.503, 0.068908, -0.41683, 0.081836,...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.075485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>loud</td>\n",
       "      <td>loud</td>\n",
       "      <td>1</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.190425</td>\n",
       "      <td>0.466753</td>\n",
       "      <td>[0.065564, 0.49259, -0.4951, -0.3633, 0.63857,...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.986178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>dishes</td>\n",
       "      <td>dish</td>\n",
       "      <td>1</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.425516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[-0.51709, -0.24079, 0.26067, -0.51739, 0.2117...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>1.871516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>pies</td>\n",
       "      <td>pie</td>\n",
       "      <td>1</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.408622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[-0.18741, -0.18939, 0.18565, -0.33282, -0.173...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>1.848622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "      <td>2</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.040916</td>\n",
       "      <td>0.695139</td>\n",
       "      <td>[0.13949, 0.53453, -0.25247, -0.12565, 0.04874...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.642056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Food</td>\n",
       "      <td>food</td>\n",
       "      <td>2</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.093827</td>\n",
       "      <td>0.445819</td>\n",
       "      <td>[-0.43512, 0.028351, 0.4911, -0.35168, -0.1157...</td>\n",
       "      <td>16.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.565646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Trucks</td>\n",
       "      <td>truck</td>\n",
       "      <td>1</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.173801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.38399, 0.22356, 0.13506, -0.55394, 0.56459,...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>1.490801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mother</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.032301</td>\n",
       "      <td>0.279935</td>\n",
       "      <td>[0.012832, 0.22669, -0.17511, 0.60248, 0.09656...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1.470236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vans</td>\n",
       "      <td>van</td>\n",
       "      <td>1</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.097780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.61935, 0.19209, 0.66574, -0.72618, 0.57477,...</td>\n",
       "      <td>11.5</td>\n",
       "      <td>17.5</td>\n",
       "      <td>1.330780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>drinks</td>\n",
       "      <td>drink</td>\n",
       "      <td>1</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.074163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[-0.21015, -0.15989, 0.46842, -0.45263, 0.0377...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>1.268163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>best</td>\n",
       "      <td>good</td>\n",
       "      <td>2</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.018684</td>\n",
       "      <td>0.279935</td>\n",
       "      <td>[-0.51704, 0.32636, 0.13205, -0.2452, 0.45463,...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1.090619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Let</td>\n",
       "      <td>let</td>\n",
       "      <td>1</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.015846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.09325, 0.050599, -0.48968, 0.27851, 0.15286...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.984846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>keeps</td>\n",
       "      <td>keep</td>\n",
       "      <td>1</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.014799</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.013511, 0.026166, -0.26397, -0.30781, -0.22...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.974799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new</td>\n",
       "      <td>new</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.34046, 0.13752, -0.20643, -0.4555, 0.19251,...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.753480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      token    lemma  appearance  difficulty  relative freqency  \\\n",
       "14    tasty    tasty           1       0.551           1.000000   \n",
       "19    pasta    pasta           1       0.543           0.852215   \n",
       "21    bakes     bake           1       0.519           0.912728   \n",
       "16  healthy  healthy           2       0.441           0.409298   \n",
       "3       car      car           4       0.342           0.375376   \n",
       "10     cook     cook           2       0.430           0.577998   \n",
       "12    meals     meal           2       0.448           0.836605   \n",
       "1      ride     ride           1       0.411           0.097780   \n",
       "4     drive    drive           2       0.379           0.200662   \n",
       "6      loud     loud           1       0.443           0.190425   \n",
       "17   dishes     dish           1       0.482           0.425516   \n",
       "20     pies      pie           1       0.480           0.408622   \n",
       "5      love     love           2       0.302           0.040916   \n",
       "11     Food     food           2       0.342           0.093827   \n",
       "7    Trucks    truck           1       0.439           0.173801   \n",
       "15   mother   mother           1       0.386           0.032301   \n",
       "8      vans      van           1       0.411           0.097780   \n",
       "13   drinks    drink           1       0.398           0.074163   \n",
       "9      best     good           2       0.264           0.018684   \n",
       "0       Let      let           1       0.323           0.015846   \n",
       "18    keeps     keep           1       0.320           0.014799   \n",
       "2       new      new           1       0.250           0.003480   \n",
       "\n",
       "    keyword score                                        word vector  \\\n",
       "14       0.362640  [-0.5076, -0.32474, 0.44828, 0.18926, 0.017181...   \n",
       "19       0.259213  [-0.47621, -0.12565, 0.57201, -0.41622, 0.0878...   \n",
       "21       0.000000  [-0.027228, 0.17126, 0.30655, -0.4484, -0.0939...   \n",
       "16       0.656656  [-0.40781, 0.53036, -0.25297, 0.19766, -0.1103...   \n",
       "3        0.898871  [0.20987, 0.46481, -0.24238, -0.065751, 0.6085...   \n",
       "10       0.314925  [-0.34052, -0.24223, 0.5442, -0.069727, 0.0716...   \n",
       "12       0.000000  [-0.32314, -0.24613, 0.41368, -0.28846, -0.097...   \n",
       "1        0.764928  [0.39289, -0.088632, -0.30353, -0.47467, 0.313...   \n",
       "4        0.737823  [0.57215, 0.503, 0.068908, -0.41683, 0.081836,...   \n",
       "6        0.466753  [0.065564, 0.49259, -0.4951, -0.3633, 0.63857,...   \n",
       "17       0.000000  [-0.51709, -0.24079, 0.26067, -0.51739, 0.2117...   \n",
       "20       0.000000  [-0.18741, -0.18939, 0.18565, -0.33282, -0.173...   \n",
       "5        0.695139  [0.13949, 0.53453, -0.25247, -0.12565, 0.04874...   \n",
       "11       0.445819  [-0.43512, 0.028351, 0.4911, -0.35168, -0.1157...   \n",
       "7        0.000000  [0.38399, 0.22356, 0.13506, -0.55394, 0.56459,...   \n",
       "15       0.279935  [0.012832, 0.22669, -0.17511, 0.60248, 0.09656...   \n",
       "8        0.000000  [0.61935, 0.19209, 0.66574, -0.72618, 0.57477,...   \n",
       "13       0.000000  [-0.21015, -0.15989, 0.46842, -0.45263, 0.0377...   \n",
       "9        0.279935  [-0.51704, 0.32636, 0.13205, -0.2452, 0.45463,...   \n",
       "0        0.000000  [0.09325, 0.050599, -0.48968, 0.27851, 0.15286...   \n",
       "18       0.000000  [0.013511, 0.026166, -0.26397, -0.30781, -0.22...   \n",
       "2        0.000000  [0.34046, 0.13752, -0.20643, -0.4555, 0.19251,...   \n",
       "\n",
       "    difficulty_rank  keyword_rank  overall_ranking  \n",
       "14              1.0           8.0         3.015640  \n",
       "19              2.0          12.0         2.740427  \n",
       "21              3.0          17.5         2.469728  \n",
       "16              8.0           5.0         2.388954  \n",
       "3              16.5           1.0         2.300247  \n",
       "10             10.0           9.0         2.182923  \n",
       "12              6.0          17.5         2.180605  \n",
       "1              11.5           2.0         2.095708  \n",
       "4              15.0           3.0         2.075485  \n",
       "6               7.0           6.0         1.986178  \n",
       "17              4.0          17.5         1.871516  \n",
       "20              5.0          17.5         1.848622  \n",
       "5              20.0           4.0         1.642056  \n",
       "11             16.5           7.0         1.565646  \n",
       "7               9.0          17.5         1.490801  \n",
       "15             14.0          10.5         1.470236  \n",
       "8              11.5          17.5         1.330780  \n",
       "13             13.0          17.5         1.268163  \n",
       "9              21.0          10.5         1.090619  \n",
       "0              18.0          17.5         0.984846  \n",
       "18             19.0          17.5         0.974799  \n",
       "2              22.0          17.5         0.753480  "
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=['overall_ranking'], ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Deep learning (also known as deep structured learning) is part of a broader family \n",
    "of machine learning methods based on artificial neural networks with representation \n",
    "        learning. Learning can be supervised, semi-supervised or unsupervised.[1][2][3]Deep-learning \n",
    "        architectures such as deep neural networks, deep belief networks, recurrent neural networks \n",
    "        and convolutional neural networks have been applied to fields including computer vision, \n",
    "        machine vision, speech recognition, natural language processing, audio recognition, social \n",
    "        network filtering, machine translation, bioinformatics, drug design, medical image analysis, \n",
    "        material inspection and board game programs, where they have produced results comparable \n",
    "        to and in some cases surpassing human expert performance.[4][5][6]Artificial neural networks \n",
    "        (ANNs) were inspired by information processing and distributed communication nodes in biological \n",
    "        systems. ANNs have various differences from biological brains. Specifically, neural networks tend \n",
    "        to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) \n",
    "        and analogue.[7][8][9]The adjective \"deep\" in deep learning refers to the use of multiple layers in \n",
    "        the network. Early work showed that a linear perceptron cannot be a universal classifier, and then \n",
    "        that a network with a nonpolynomial activation function with one hidden layer of unbounded width \n",
    "        can on the other hand so be. Deep learning is a modern variation which is concerned with an unbounded\n",
    "        number of layers of bounded size, which permits practical application and optimized implementation, \n",
    "        while retaining theoretical universality under mild conditions. In deep learning the layers are \n",
    "        also permitted to be heterogeneous and to deviate widely from biologically informed connectionist \n",
    "        models, for the sake of efficiency, trainability and understandability, whence the \"structured\" part.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257 words in document\n",
      "152 words in vocabulary\n",
      "110 words in vocabulary without duplicates\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_from_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['overall_ranking'], ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word position 0:\n",
      "Can you translate this word:   nonpolynomial   [y/n]  n\n",
      "Word position 3:\n",
      "Can you translate this word:   bioinformatic   [y/n]  n\n",
      "Word position 7:\n",
      "Can you translate this word:   neural   [y/n]  n\n",
      "Word position 15:\n",
      "Can you translate this word:   supervised   [y/n]  n\n",
      "Word position 31:\n",
      "Can you translate this word:   heterogeneous   [y/n]  n\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>appearance</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>relative freqency</th>\n",
       "      <th>keyword score</th>\n",
       "      <th>difficulty_rank</th>\n",
       "      <th>keyword_rank</th>\n",
       "      <th>overall_ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>nonpolynomial</td>\n",
       "      <td>nonpolynomial</td>\n",
       "      <td>1</td>\n",
       "      <td>0.889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.348088</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.5</td>\n",
       "      <td>4.015088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>trainability</td>\n",
       "      <td>trainability</td>\n",
       "      <td>1</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.934694</td>\n",
       "      <td>0.349549</td>\n",
       "      <td>3.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>3.486243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>understandability</td>\n",
       "      <td>understandability</td>\n",
       "      <td>1</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.724685</td>\n",
       "      <td>0.349549</td>\n",
       "      <td>5.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>3.240234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>bioinformatics</td>\n",
       "      <td>bioinformatic</td>\n",
       "      <td>1</td>\n",
       "      <td>0.738</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>87.5</td>\n",
       "      <td>3.214000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>convolutional</td>\n",
       "      <td>convolutional</td>\n",
       "      <td>1</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.489316</td>\n",
       "      <td>0.521135</td>\n",
       "      <td>7.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.119451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>unbounded</td>\n",
       "      <td>unbounded</td>\n",
       "      <td>2</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.270206</td>\n",
       "      <td>0.713559</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.098765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>connectionist</td>\n",
       "      <td>connectionist</td>\n",
       "      <td>1</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.741101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>87.5</td>\n",
       "      <td>2.910101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>neural</td>\n",
       "      <td>neural</td>\n",
       "      <td>6</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.197697</td>\n",
       "      <td>0.960700</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.868397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>artificial</td>\n",
       "      <td>artificial</td>\n",
       "      <td>1</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.001449</td>\n",
       "      <td>0.912344</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.491793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>recurrent</td>\n",
       "      <td>recurrent</td>\n",
       "      <td>1</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.009786</td>\n",
       "      <td>0.564295</td>\n",
       "      <td>14.5</td>\n",
       "      <td>20.5</td>\n",
       "      <td>2.410081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>biological</td>\n",
       "      <td>biological</td>\n",
       "      <td>3</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>0.863060</td>\n",
       "      <td>36.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.408060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>classifier</td>\n",
       "      <td>classifier</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.107009</td>\n",
       "      <td>0.388546</td>\n",
       "      <td>11.0</td>\n",
       "      <td>46.5</td>\n",
       "      <td>2.385555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>structured</td>\n",
       "      <td>structured</td>\n",
       "      <td>2</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.012652</td>\n",
       "      <td>0.736233</td>\n",
       "      <td>25.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.377885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>filtering</td>\n",
       "      <td>filtering</td>\n",
       "      <td>1</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.006755</td>\n",
       "      <td>0.564295</td>\n",
       "      <td>17.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>2.353050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>representation</td>\n",
       "      <td>representation</td>\n",
       "      <td>1</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.790779</td>\n",
       "      <td>37.5</td>\n",
       "      <td>10.5</td>\n",
       "      <td>2.315691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>supervised</td>\n",
       "      <td>supervised</td>\n",
       "      <td>1</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.005241</td>\n",
       "      <td>0.564295</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>2.315536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>recognition</td>\n",
       "      <td>recognition</td>\n",
       "      <td>2</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>0.825955</td>\n",
       "      <td>48.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2.298656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>adjective</td>\n",
       "      <td>adjective</td>\n",
       "      <td>1</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.011450</td>\n",
       "      <td>0.420414</td>\n",
       "      <td>12.5</td>\n",
       "      <td>44.0</td>\n",
       "      <td>2.288864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>processing</td>\n",
       "      <td>processing</td>\n",
       "      <td>2</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.003405</td>\n",
       "      <td>0.789558</td>\n",
       "      <td>44.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.274962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>learning</td>\n",
       "      <td>learning</td>\n",
       "      <td>7</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.012882</td>\n",
       "      <td>0.944046</td>\n",
       "      <td>75.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.225928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>vision</td>\n",
       "      <td>vision</td>\n",
       "      <td>2</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>0.825955</td>\n",
       "      <td>59.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2.189959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>machine</td>\n",
       "      <td>machine</td>\n",
       "      <td>3</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.002595</td>\n",
       "      <td>0.902369</td>\n",
       "      <td>73.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.188964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Deep</td>\n",
       "      <td>deep</td>\n",
       "      <td>8</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.015001</td>\n",
       "      <td>0.955717</td>\n",
       "      <td>91.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.140719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>networks</td>\n",
       "      <td>network</td>\n",
       "      <td>10</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.020818</td>\n",
       "      <td>0.873967</td>\n",
       "      <td>79.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.130785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>inspection</td>\n",
       "      <td>inspection</td>\n",
       "      <td>1</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.001735</td>\n",
       "      <td>0.521135</td>\n",
       "      <td>30.5</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2.106870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>comparable</td>\n",
       "      <td>comparable</td>\n",
       "      <td>1</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.001735</td>\n",
       "      <td>0.521135</td>\n",
       "      <td>30.5</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2.106870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>activation</td>\n",
       "      <td>activation</td>\n",
       "      <td>1</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.004075</td>\n",
       "      <td>0.348088</td>\n",
       "      <td>19.0</td>\n",
       "      <td>60.5</td>\n",
       "      <td>2.080163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>symbolic</td>\n",
       "      <td>1</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.002753</td>\n",
       "      <td>0.420414</td>\n",
       "      <td>23.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>2.076167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>universality</td>\n",
       "      <td>universality</td>\n",
       "      <td>1</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.028168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>87.5</td>\n",
       "      <td>2.053168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>translation</td>\n",
       "      <td>translation</td>\n",
       "      <td>1</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.564295</td>\n",
       "      <td>45.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>2.041107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 token              lemma  appearance  difficulty  \\\n",
       "78       nonpolynomial      nonpolynomial           1       0.889   \n",
       "108       trainability       trainability           1       0.734   \n",
       "109  understandability  understandability           1       0.722   \n",
       "33      bioinformatics      bioinformatic           1       0.738   \n",
       "19       convolutional      convolutional           1       0.703   \n",
       "82           unbounded          unbounded           2       0.705   \n",
       "104      connectionist      connectionist           1       0.723   \n",
       "10              neural             neural           6       0.570   \n",
       "9           artificial         artificial           1       0.526   \n",
       "18           recurrent          recurrent           1       0.612   \n",
       "56          biological         biological           3       0.512   \n",
       "77          classifier         classifier           1       0.630   \n",
       "3           structured         structured           2       0.543   \n",
       "31           filtering          filtering           1       0.594   \n",
       "12      representation     representation           1       0.508   \n",
       "14          supervised         supervised           1       0.582   \n",
       "26         recognition        recognition           2       0.490   \n",
       "67           adjective          adjective           1       0.619   \n",
       "29          processing         processing           2       0.494   \n",
       "1             learning           learning           7       0.423   \n",
       "24              vision             vision           2       0.454   \n",
       "6              machine            machine           3       0.428   \n",
       "0                 Deep               deep           8       0.390   \n",
       "11            networks            network          10       0.412   \n",
       "40          inspection         inspection           1       0.528   \n",
       "46          comparable         comparable           1       0.528   \n",
       "79          activation         activation           1       0.576   \n",
       "62            symbolic           symbolic           1       0.551   \n",
       "98        universality       universality           1       0.675   \n",
       "32         translation        translation           1       0.492   \n",
       "\n",
       "     relative freqency  keyword score  difficulty_rank  keyword_rank  \\\n",
       "78            1.000000       0.348088              1.0          60.5   \n",
       "108           0.934694       0.349549              3.0          55.0   \n",
       "109           0.724685       0.349549              5.0          55.0   \n",
       "33            1.000000       0.000000              2.0          87.5   \n",
       "19            0.489316       0.521135              7.0          34.0   \n",
       "82            0.270206       0.713559              6.0          14.0   \n",
       "104           0.741101       0.000000              4.0          87.5   \n",
       "10            0.197697       0.960700             21.0           1.0   \n",
       "9             0.001449       0.912344             32.0           4.0   \n",
       "18            0.009786       0.564295             14.5          20.5   \n",
       "56            0.009000       0.863060             36.0           7.0   \n",
       "77            0.107009       0.388546             11.0          46.5   \n",
       "3             0.012652       0.736233             25.0          13.0   \n",
       "31            0.006755       0.564295             17.0          20.5   \n",
       "12            0.000912       0.790779             37.5          10.5   \n",
       "14            0.005241       0.564295             18.0          20.5   \n",
       "26            0.002701       0.825955             48.0           8.5   \n",
       "67            0.011450       0.420414             12.5          44.0   \n",
       "29            0.003405       0.789558             44.0          12.0   \n",
       "1             0.012882       0.944046             75.0           3.0   \n",
       "24            0.002004       0.825955             59.0           8.5   \n",
       "6             0.002595       0.902369             73.0           5.0   \n",
       "0             0.015001       0.955717             91.5           2.0   \n",
       "11            0.020818       0.873967             79.0           6.0   \n",
       "40            0.001735       0.521135             30.5          34.0   \n",
       "46            0.001735       0.521135             30.5          34.0   \n",
       "79            0.004075       0.348088             19.0          60.5   \n",
       "62            0.002753       0.420414             23.0          44.0   \n",
       "98            0.028168       0.000000              8.0          87.5   \n",
       "32            0.000812       0.564295             45.0          20.5   \n",
       "\n",
       "     overall_ranking  \n",
       "78          4.015088  \n",
       "108         3.486243  \n",
       "109         3.240234  \n",
       "33          3.214000  \n",
       "19          3.119451  \n",
       "82          3.098765  \n",
       "104         2.910101  \n",
       "10          2.868397  \n",
       "9           2.491793  \n",
       "18          2.410081  \n",
       "56          2.408060  \n",
       "77          2.385555  \n",
       "3           2.377885  \n",
       "31          2.353050  \n",
       "12          2.315691  \n",
       "14          2.315536  \n",
       "26          2.298656  \n",
       "67          2.288864  \n",
       "29          2.274962  \n",
       "1           2.225928  \n",
       "24          2.189959  \n",
       "6           2.188964  \n",
       "0           2.140719  \n",
       "11          2.130785  \n",
       "40          2.106870  \n",
       "46          2.106870  \n",
       "79          2.080163  \n",
       "62          2.076167  \n",
       "98          2.053168  \n",
       "32          2.041107  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_outputsize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TextAnalytics2.2] *",
   "language": "python",
   "name": "conda-env-TextAnalytics2.2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
